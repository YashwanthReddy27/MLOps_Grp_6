name: Simple RAG Model CI/CD

on:
  push:
    branches:
      - feat/model-building
      - main
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  GCP_LOCATION: 'us-central1'
  GCP_REPOSITORY: 'rag-models'

jobs:
  evaluate-and-push:
    name: Evaluate Model & Push to Registry
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r researchAI/model/requirements.txt

      - name: Download NLTK data
        run: |
          python -c "import nltk; nltk.download('punkt'); nltk.download('punkt_tab'); nltk.download('stopwords')"

      - name: Create GCP credentials file
        env:
          GCP_SA_DVC_KEY: ${{ secrets.GCP_SA_DVC_KEY }}
        run: echo "$GCP_SA_DVC_KEY" > gcp-credentials.json
      - name: Dump
        run: echo "$GCP_SA_DVC_KEY"
      - name: Configure DVC remote
        run: |
          dvc remote add -f myremote ${{ secrets.GCP_DVC_BUCKET }}
          dvc remote modify --local myremote credentialpath "$PWD/gcp-credentials.json"

      - name: DVC fetch data
        run: |
          dvc pull
          echo "Trying to run DVC Pull"

      - name: Run Indexing Script
        env:
          PYTHONPATH: ${{ github.workspace }}/researchAI/model
        run: |
          cd researchAI/model
          python indexing_script.py
          cd ../..
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
        continue-on-error: true
      
      - name: Set up Google Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}
        continue-on-error: true
      
      - name: Download previous evaluation report
        continue-on-error: true
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: simple_model_cicd.yml
          name: evaluation-report
          path: ./previous_reports/
          search_artifacts: true
          if_no_artifact_found: warn
      
      - name: Run Complete Evaluation Pipeline
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          PYTHONPATH: ${{ github.workspace }}/researchAI/model
        run: |
          cd researchAI/model
          
          PREVIOUS_REPORT=""
          if [ -f ../../previous_reports/evaluation_report.json ]; then
            PREVIOUS_REPORT="--previous-report ../../previous_reports/evaluation_report.json"
            echo "‚úÖ Found previous report for comparison"
          else
            echo "‚ÑπÔ∏è  No previous report - first run"
          fi
          
          PUSH_FLAG="--push-to-registry"
          echo "üöÄ Will push to Artifact Registry if passed"
          
          python evaluation/simple_evaluate.py \
            --test-data test_queries_with_expected.csv \
            --output evaluation_report.json \
            --validation-threshold 0.7 \
            --fairness-threshold 0.6 \
            $PUSH_FLAG \
            --project-id ${{ env.GCP_PROJECT_ID }} \
            --location ${{ env.GCP_LOCATION }} \
            --repository ${{ env.GCP_REPOSITORY }} \
            --version "v$(date +%Y%m%d-%H%M%S)" \
            $PREVIOUS_REPORT
          
          if [ -f artifact_path.txt ]; then
            cp artifact_path.txt ${{ github.workspace }}/artifact_path.txt
          fi
          cp evaluation_report.json ${{ github.workspace }}/evaluation_report.json
        id: evaluation
      
      - name: Upload evaluation report
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-report
          path: evaluation_report.json
          retention-days: 90
      
      - name: Read evaluation metrics
        if: always()
        id: read_metrics
        run: |
          if [ -f evaluation_report.json ]; then
            STATUS=$(jq -r '.aggregate_metrics.status' evaluation_report.json)
            VALIDATION=$(jq -r '.aggregate_metrics.avg_validation_score' evaluation_report.json)
            FAIRNESS=$(jq -r '.aggregate_metrics.avg_fairness_score' evaluation_report.json)
            VAL_THRESHOLD=$(jq -r '.aggregate_metrics.thresholds.validation' evaluation_report.json)
            FAIR_THRESHOLD=$(jq -r '.aggregate_metrics.thresholds.fairness' evaluation_report.json)
            VAL_PASS_RATE=$(jq -r '.aggregate_metrics.validation_pass_rate' evaluation_report.json)
            FAIR_PASS_RATE=$(jq -r '.aggregate_metrics.fairness_pass_rate' evaluation_report.json)
            
            echo "status=$STATUS" >> $GITHUB_OUTPUT
            echo "validation=$VALIDATION" >> $GITHUB_OUTPUT
            echo "fairness=$FAIRNESS" >> $GITHUB_OUTPUT
            echo "val_threshold=$VAL_THRESHOLD" >> $GITHUB_OUTPUT
            echo "fair_threshold=$FAIR_THRESHOLD" >> $GITHUB_OUTPUT
            echo "val_pass_rate=$VAL_PASS_RATE" >> $GITHUB_OUTPUT
            echo "fair_pass_rate=$FAIR_PASS_RATE" >> $GITHUB_OUTPUT
            
            # Calculate gaps
            VAL_GAP=$(echo "$VAL_THRESHOLD - $VALIDATION" | bc)
            FAIR_GAP=$(echo "$FAIR_THRESHOLD - $FAIRNESS" | bc)
            echo "val_gap=$VAL_GAP" >> $GITHUB_OUTPUT
            echo "fair_gap=$FAIR_GAP" >> $GITHUB_OUTPUT
          else
            echo "status=FAILED" >> $GITHUB_OUTPUT
            echo "validation=0.0" >> $GITHUB_OUTPUT
            echo "fairness=0.0" >> $GITHUB_OUTPUT
            echo "val_threshold=0.7" >> $GITHUB_OUTPUT
            echo "fair_threshold=0.6" >> $GITHUB_OUTPUT
            echo "val_gap=0.7" >> $GITHUB_OUTPUT
            echo "fair_gap=0.6" >> $GITHUB_OUTPUT
          fi
      
      - name: Send Failure Email
        if: failure()
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 587
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: "‚ùå RAG Model Evaluation FAILED - ${{ github.ref_name }}"
          to: ${{ secrets.DEVELOPER_EMAIL }}
          from: RAG Model CI/CD
          body: |
            ‚ùå RAG Model Evaluation FAILED
            
            Branch: ${{ github.ref_name }}
            Commit: ${{ github.sha }}
            
            üìä METRICS THAT FAILED:
            
            Validation Score:
              ‚Ä¢ Achieved: ${{ steps.read_metrics.outputs.validation }}
              ‚Ä¢ Required: ${{ steps.read_metrics.outputs.val_threshold }}
              ‚Ä¢ Gap: ${{ steps.read_metrics.outputs.val_gap }} below threshold
              ‚Ä¢ Pass Rate: ${{ steps.read_metrics.outputs.val_pass_rate }}
            
            Fairness Score:
              ‚Ä¢ Achieved: ${{ steps.read_metrics.outputs.fairness }}
              ‚Ä¢ Required: ${{ steps.read_metrics.outputs.fair_threshold }}
              ‚Ä¢ Gap: ${{ steps.read_metrics.outputs.fair_gap }} below threshold
              ‚Ä¢ Pass Rate: ${{ steps.read_metrics.outputs.fair_pass_rate }}
            
            ‚ö†Ô∏è POSSIBLE REASONS FOR FAILURE:
            1. Validation score below 0.7 threshold
            2. Fairness score below 0.6 threshold
            3. Model regressed compared to previous version
            4. Less than 80% of queries passed validation checks
            5. Less than 80% of queries passed fairness checks
            
            üîß NEXT STEPS:
            ‚Ä¢ Review the evaluation report artifact in GitHub Actions
            ‚Ä¢ Check if indexes are properly loaded
            ‚Ä¢ Verify test queries in test_queries_with_expected.csv
            ‚Ä¢ Improve retrieval or generation parameters if needed
            
            üîó View Full Details:
            ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
        continue-on-error: true
