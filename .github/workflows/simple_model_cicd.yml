name: Simple RAG Model CI/CD

on:
  push:
    branches:
      - feat/model-building
      - main
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  GCP_PROJECT_ID: ${{ secrets.GCP_ARTIFACT_PROJECT_ID }}  # Project for Artifact Registry
  GCP_LOCATION: 'us-central1'
  GCP_REPOSITORY: 'rag-models'

jobs:
  evaluate-and-push:
    name: Evaluate Model & Push to Registry
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r researchAI/model/requirements.txt

      - name: Download NLTK data
        run: |
          python -c "import nltk; nltk.download('punkt'); nltk.download('punkt_tab'); nltk.download('stopwords')"

      # ============================================
      # STEP 1: Setup DVC Authentication (Account A)
      # ============================================
      - name: Create DVC GCP credentials file
        env:
          GCP_SA_DVC_KEY: ${{ secrets.GCP_SA_DVC_KEY }}
        run: |
          echo "$GCP_SA_DVC_KEY" > $HOME/dvc-credentials.json
          echo "‚úÖ DVC credentials file created"

      - name: Authenticate DVC to Google Cloud
        run: |
          gcloud auth activate-service-account --key-file=$HOME/dvc-credentials.json
          echo "‚úÖ DVC service account authenticated"

      - name: Configure DVC remote
        run: |
          dvc remote modify myremote url ${{ secrets.GCP_DVC_BUCKET }}
          echo "‚úÖ DVC remote configured"
          
      - name: DVC pull data
        run: |
          dvc pull
          echo "‚úÖ DVC pull completed successfully"

      # ============================================
      # STEP 2: Switch to Artifact Registry Authentication (Account B)
      # ============================================
      - name: Create Artifact Registry GCP credentials file
        env:
          GCP_SA_ARTIFACT_KEY: ${{ secrets.GCP_SA_ARTIFACT_KEY }}
        run: |
          echo "$GCP_SA_ARTIFACT_KEY" > $HOME/artifact-credentials.json
          echo "‚úÖ Artifact Registry credentials file created"

      - name: Authenticate to Artifact Registry Google Cloud
        run: |
          gcloud auth activate-service-account --key-file=$HOME/artifact-credentials.json
          gcloud config set project ${{ secrets.GCP_ARTIFACT_PROJECT_ID }}
          echo "‚úÖ Artifact Registry service account authenticated"
      
      - name: Verify Artifact Registry Authentication
        run: |
          gcloud auth list
          gcloud config list project
          echo "‚úÖ Authentication verification complete"

      # ============================================
      # STEP 3: Run Model Pipeline
      # ============================================
      - name: Run Indexing Script
        env:
          PYTHONPATH: ${{ github.workspace }}/researchAI/model
        run: |
          cd researchAI/model
          python indexing_script.py
          cd ../..
      
      - name: Download previous evaluation report
        continue-on-error: true
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: simple_model_cicd.yml
          name: evaluation-report
          path: ./previous_reports/
          search_artifacts: true
          if_no_artifact_found: warn
      
      - name: Run Complete Evaluation Pipeline
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          PYTHONPATH: ${{ github.workspace }}/researchAI/model
          # Make Artifact Registry project ID available to Python code
          GCP_PROJECT_ID: ${{ secrets.GCP_ARTIFACT_PROJECT_ID }}
        run: |
          cd researchAI/model
          
          PREVIOUS_REPORT=""
          if [ -f ../../previous_reports/evaluation_report.json ]; then
            PREVIOUS_REPORT="--previous-report ../../previous_reports/evaluation_report.json"
            echo "‚úÖ Found previous report for comparison"
          else
            echo "‚ÑπÔ∏è  No previous report - first run"
          fi
          
          PUSH_FLAG="--push-to-registry"
          echo "üöÄ Will push to Artifact Registry if passed"
          
          python evaluation/simple_evaluate.py \
            --test-data test_queries_with_expected.csv \
            --output evaluation_report.json \
            --validation-threshold 0.7 \
            --fairness-threshold 0.6 \
            $PUSH_FLAG \
            --project-id ${{ secrets.GCP_ARTIFACT_PROJECT_ID }} \
            --location ${{ env.GCP_LOCATION }} \
            --repository ${{ env.GCP_REPOSITORY }} \
            --version "v$(date +%Y%m%d-%H%M%S)" \
            $PREVIOUS_REPORT
          
          if [ -f artifact_path.txt ]; then
            cp artifact_path.txt ${{ github.workspace }}/artifact_path.txt
          fi
          cp evaluation_report.json ${{ github.workspace }}/evaluation_report.json
        id: evaluation
      
      - name: Upload evaluation report
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-report
          path: evaluation_report.json
          retention-days: 90
      
      - name: Read evaluation metrics
        if: always()
        id: read_metrics
        run: |
          if [ -f evaluation_report.json ]; then
            STATUS=$(jq -r '.aggregate_metrics.status' evaluation_report.json)
            VALIDATION=$(jq -r '.aggregate_metrics.avg_validation_score' evaluation_report.json)
            FAIRNESS=$(jq -r '.aggregate_metrics.avg_fairness_score' evaluation_report.json)
            VAL_THRESHOLD=$(jq -r '.aggregate_metrics.thresholds.validation' evaluation_report.json)
            FAIR_THRESHOLD=$(jq -r '.aggregate_metrics.thresholds.fairness' evaluation_report.json)
            VAL_PASS_RATE=$(jq -r '.aggregate_metrics.validation_pass_rate' evaluation_report.json)
            FAIR_PASS_RATE=$(jq -r '.aggregate_metrics.fairness_pass_rate' evaluation_report.json)
            
            echo "status=$STATUS" >> $GITHUB_OUTPUT
            echo "validation=$VALIDATION" >> $GITHUB_OUTPUT
            echo "fairness=$FAIRNESS" >> $GITHUB_OUTPUT
            echo "val_threshold=$VAL_THRESHOLD" >> $GITHUB_OUTPUT
            echo "fair_threshold=$FAIR_THRESHOLD" >> $GITHUB_OUTPUT
            echo "val_pass_rate=$VAL_PASS_RATE" >> $GITHUB_OUTPUT
            echo "fair_pass_rate=$FAIR_PASS_RATE" >> $GITHUB_OUTPUT
            
            # Calculate gaps
            VAL_GAP=$(echo "$VAL_THRESHOLD - $VALIDATION" | bc)
            FAIR_GAP=$(echo "$FAIR_THRESHOLD - $FAIRNESS" | bc)
            echo "val_gap=$VAL_GAP" >> $GITHUB_OUTPUT
            echo "fair_gap=$FAIR_GAP" >> $GITHUB_OUTPUT
          else
            echo "status=FAILED" >> $GITHUB_OUTPUT
            echo "validation=0.0" >> $GITHUB_OUTPUT
            echo "fairness=0.0" >> $GITHUB_OUTPUT
            echo "val_threshold=0.7" >> $GITHUB_OUTPUT
            echo "fair_threshold=0.6" >> $GITHUB_OUTPUT
            echo "val_gap=0.7" >> $GITHUB_OUTPUT
            echo "fair_gap=0.6" >> $GITHUB_OUTPUT
          fi
      
      # ============================================
      # STEP 4: Cleanup & Notifications
      # ============================================
      - name: Cleanup credentials
        if: always()
        run: |
          rm -f $HOME/dvc-credentials.json
          rm -f $HOME/artifact-credentials.json
          echo "‚úÖ Credentials cleaned up"
      
      - name: Send Failure Email
        if: failure()
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 587
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: "‚ùå RAG Model Evaluation FAILED - ${{ github.ref_name }}"
          to: ${{ secrets.DEVELOPER_EMAIL }}
          from: RAG Model CI/CD
          body: |
            ‚ùå RAG Model Evaluation FAILED
            
            Branch: ${{ github.ref_name }}
            Commit: ${{ github.sha }}
            
            üìä METRICS THAT FAILED:
            
            Validation Score:
              ‚Ä¢ Achieved: ${{ steps.read_metrics.outputs.validation }}
              ‚Ä¢ Required: ${{ steps.read_metrics.outputs.val_threshold }}
              ‚Ä¢ Gap: ${{ steps.read_metrics.outputs.val_gap }} below threshold
              ‚Ä¢ Pass Rate: ${{ steps.read_metrics.outputs.val_pass_rate }}
            
            Fairness Score:
              ‚Ä¢ Achieved: ${{ steps.read_metrics.outputs.fairness }}
              ‚Ä¢ Required: ${{ steps.read_metrics.outputs.fair_threshold }}
              ‚Ä¢ Gap: ${{ steps.read_metrics.outputs.fair_gap }} below threshold
              ‚Ä¢ Pass Rate: ${{ steps.read_metrics.outputs.fair_pass_rate }}
            
            ‚ö†Ô∏è POSSIBLE REASONS FOR FAILURE:
            1. Validation score below 0.7 threshold
            2. Fairness score below 0.6 threshold
            3. Model regressed compared to previous version
            4. Less than 80% of queries passed validation checks
            5. Less than 80% of queries passed fairness checks
            
            üîß NEXT STEPS:
            ‚Ä¢ Review the evaluation report artifact in GitHub Actions
            ‚Ä¢ Check if indexes are properly loaded
            ‚Ä¢ Verify test queries in test_queries_with_expected.csv
            ‚Ä¢ Improve retrieval or generation parameters if needed
            
            üîó View Full Details:
            ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
        continue-on-error: true